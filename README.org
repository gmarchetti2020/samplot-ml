#+TITLE: samplot-ml
* Training Data Workflow
** 1000 genomes high coverage crams
Download the Cram indices listed on the high_coverage index
ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/1000genomes.high_coverage.GRCh38DH.alignment.index
- Aligned to GRCh38 reference genome

** Get the SV callset VCF (using GRCh38 ref)
http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/integrated_sv_map/supporting/GRCh38_positions/ALL.wgs.integrated_sv_map_v1_GRCh38.20130502.svs.genotypes.vcf.gz

** Extract DEL/Non-DEL regions for training set
#+BEGIN_SRC bash
bcftools view -i 'SVTYPE="DEL"' $VCF \
    | bcftools query -f '%CHROM\t%POS\t%INFO/END[\t%SAMPLE,%GT]\n' \ 
    | python sample_del.py > $OUT/del.sample.bed # from git repo
#+END_SRC
- Output to bed file annotated with sample and genotype (REF, HET, ALT)
- TODO make the variables command line args with flags


** Generate training images
*** gen_img.sh
#+BEGIN_SRC bash
cat $BED_FILE | gargs -p $PROCESSES \
 "bash gen_img.sh \\
     --chrom {0} --start {1} --end {2} --sample {3} --genotype {4} \\
     --fasta $FASTA \\
     --bam-list $CRAM_LIST \\
     --bam-dir $CRAM_INDEX_DIR \\
     --out-dir $OUT_DIR/imgs"
#+END_SRC
- Use [[https://github.com/brentp/gargs][gargs]] to parse the contents of the Training regions and feed to =gen_img.sh=
- =$CRAM_LIST= (they're actually crams) can be absolute file paths or urls (must download indices though)
- =$CRAM_DIR= is the directory containing the CRAM indices
- TODO upload the cram list of urls to github
- TODO check if I used a different GRCh38 from "full_analysis_set_plut_decoy_hla"

** Crop images to remove the surrounding text/axes
#+BEGIN_SRC bash
bash crop.sh \
    --processes $NUM_PROCESSES \
    --data-dir $DATA_DIR
#+END_SRC
- Where =$DATA_DIR= is the parent directory containing the img/ directory from
  the previous step
- Cropped images will be placed in =$DATA_DIR/crop=
  

** TODO Split into train/val sets (file listings)
  
* Training Procedure
We use the =run.py= script to train a new model

#+BEGIN_SRC 
usage: run.py train [-h] [--batch-size BATCH_SIZE] [--epochs EPOCHS]
                    [--model-type MODEL_TYPE] --data-dir DATA_DIR
                    [--learning-rate LR] [--momentum MOMENTUM]
                    [--label-smoothing LABEL_SMOOTHING] [--save-to SAVE_TO]

optional arguments:
  -h, --help            show this help message and exit
  --batch-size BATCH_SIZE, -b BATCH_SIZE
                        Number of images to feed to model at a time. (default:
                        80)
  --epochs EPOCHS, -e EPOCHS
                        Max number of epochs to train model. (default: 100)
  --model-type MODEL_TYPE, -mt MODEL_TYPE
                        Type of model to train. (default: CNN)
  --data-dir DATA_DIR, -d DATA_DIR
                        Root directory of the training data. (default: None)
  --learning-rate LR, -lr LR
                        Learning rate for optimizer. (default: 0.0001)
  --momentum MOMENTUM, -mom MOMENTUM
                        Momentum term in SGD optimizer. (default: 0.9)
  --label-smoothing LABEL_SMOOTHING, -ls LABEL_SMOOTHING
                        Strength of label smoothing (0-1). (default: 0.0)
  --save-to SAVE_TO, -s SAVE_TO
                        filename if you want to save your trained model.
                        (default: None)
#+END_SRC


* Test data workflow
** HG002 data processing
*** TODO Get Cram/index

*** TODO Get VCF and tier 1 regions bed
ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_SVs_Integration_v0.6/HG002_SVs_Tier1_v0.6.vcf.gz
ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_SVs_Integration_v0.6/HG002_SVs_Tier1_v0.6.bed
**** Filter DELs with =bcftools view=

** HG00514, HG00733, NA19240 data processing
*** Get Crams/indices
**** HG00514
ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/hgsv_sv_discovery/data/CHS/HG00514/high_cov_alignment/
**** HG00733
ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/hgsv_sv_discovery/data/PUR/HG00733/high_cov_alignment/
**** NA19240
ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/hgsv_sv_discovery/data/YRI/NA19240/high_cov_alignment/

*** DONE Get truth set VCFs/indices
ftp://ftp.ncbi.nlm.nih.gov/pub/dbVar/data/Homo_sapiens/by_study/genotype/nstd152
**** Filter DELs with =bcftools view= and Fix VCFs
- Remove length 0 contigs (causes problems with truvari otherwise)
- Run =fix_vcf.py= script to correct SVLEN
  - For some reason the %INFO/END field is just start + 1 so we need to
    use SVLEN to calculate the true end.
    
#+BEGIN_SRC bash
bcftools view -i 'SVTYPE="DEL"' $TRUTH_SET_VCF \
    | grep -v "length=0>" \
    | python fix_vcf.py \
    | bgzip -c > $FIXED_TRUTH_SET
tabix $FIXED_TRUTH_SET
#+END_SRC

*** Genotype with smoove (annotated with duphold) to get baseline VCF
Use the following command
#+BEGIN_SRC bash
smoove call \
    --outdir $OUT_DIR \
    --processes $PROCESSES \
    --name $SAMPLE_NAME \ # eg HG00514
    --exclude $BED_DIR/exclude.cnvnator_100bp.GRCh38.20170403.bed
    --fasta $FASTA # 
    --removepr \
    --genotype \
    --duphold \
    $CRAM_PATH
#+END_SRC

You can get the exclude regions bed for GRCh38 from [[https://github.com/hall-lab/speedseq/blob/master/annotations/exclude.cnvnator_100bp.GRCh38.20170403.bed][here]]

**** Use GRCh38_full_analysis_set_plus_decoy_hla.fa reference genome
[[ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa][fasta]]
[[ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa.fai][index]]

*** Generate images
**** From smoove generated VCF, extract just the dels
#+BEGIN_SRC bash
bcftools view -i 'SVTYPE="DEL"' $SAMPLE-smoove.genotyped.vcf.gz \
    | bgzip -c > $SAMPLE-smoove.genotyped.del.vcf.gz
#+END_SRC
**** TODO from VCF to bed, pipe to gargs, call gen_img.sh
#+BEGIN_SRC bash
bcftools query -f '%CHROM\t%POS\t%INFO/END[\t%SAMPLE\t%GT]\n' \
    $SAMPLE-smoove.genotyped.del.vcf.gz  | gargs -p $PROCESSES \
    "bash gen_img.sh \\
        --chrom {0} --start {1} --end {2} --sample $SAMPLE --genotype DEL \\
        --fasta $FASTA \\
        --bam-dir $PATH_TO_CRAM \\
        --out-dir $OUT_DIR/imgs"
#+END_SRC

*** TODO Crop images
#+BEGIN_SRC bash
bash crop.sh \
    --processes $NUM_PROCESSES \
    --data-dir $DATA_DIR
#+END_SRC
- Where =$DATA_DIR= is the parent directory containing the img/ directory from
  the previous step
- Cropped images will be placed in =$DATA_DIR/crop=
  
*** Create file listing for images
#+BEGIN_SRC bash
cd $SAMPLE_DIR # parent directory of cropped images
find $(pwd)/crop/*.png > $IMAGE_LIST
#+END_SRC

*** Filter using duphold annotations
#+BEGIN_SRC bash
bcftools view -i 'DHFFC<0.7' $BASELINE_VCF | bgzip -c > dhffc.lt.0.7.vcf.gz
tabix dhffc.lt.0.7.vcf.gz
#+END_SRC

*** TODO Filter with CNN model
#+BEGIN_SRC bash
bash create_test_vcfs.sh \
    --model-path $MODEL_PATH \
    --data-list $IMAGE_LIST \
    --vcf $BASELINE_VCF \ # i.e. the smoove genotyped vcf
    --out-dir $OUT_DIR
#+END_SRC

*** TODO Run truvari on baseline, duphold and CNN VCF
#+BEGIN_SRC bash
bash truvari.sh \
    --comp-vcf $COMP_VCF \
    --base-vcf $TRUTH_SET_VCF \
    --reference $REF \
    --out-dir $OUT_DIR
#+END_SRC

* Results/Analysis
*** TODO Truvari statistics
- [ ] Total events from truth set
- [ ] tp/fp/fn, precision/recall/f1 for all methods
*** TODO Size distribution on truth set
*** TODO TP and FP size distribution for baseline, duphold and CNN
*** TODO TP and FP chromosome distribution?
*** TODO Venn digram of truth set, baseline, duphold, CNN false positives

** Questions
*** Does CNN/duphold make the same/different mistakes (Venn digram)?
*** What is duphold good/bad at?
- Grad cam visualizations of the ML true/false positives
*** What is the size distribution of duphold/CNN fp intersection/difference
*** What is the duphold score distribution for CNN tp/fp
*** What are the ML scores for duphold tp/fp
